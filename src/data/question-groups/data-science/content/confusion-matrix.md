A confusion matrix is a simple table that shows how well a classification model is performing by comparing its predictions to the actual results. It breaks down the predictions into four categories: correct predictions for both classes (true positives and true negatives) and incorrect predictions (false positives and false negatives). This helps you understand where the model is making mistakes so you can improve it.

- **TP:** True Positives
- **TN:** True Negatives
- **FP:** False Positives
- **FN:** False Negatives

![Confusion matrix](https://assets.roadmap.sh/guest/actual-values-qnezf.png)

**Example:** From the matrix below, there are 165 total cases:

- **True Negatives:** **50**
- **False Positives:** **10**
- **False Negatives:** **5**
- **True Positives:** **100**

Depending on the project you're working on, you can use metrics such as Accuracy, Precision, Recall, and F1 Score to evaluate your project. A confusion matrix is a visualization of it.

![An example of confusion matrix](https://assets.roadmap.sh/guest/sample-confusion-matrix-l4q8i.png)

**Common pitfall:** Misreading the matrix layout or assuming it works for multi-class without modification. 